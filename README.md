# SSE Study

---

## 📖 프로젝트 소개

이 프로젝트는 Server-Sent Events(SSE)를 활용한 이벤트 처리 및 알림 기능을 테스트하고, 그 결과를 기록하기 위한 스터디 프로젝트입니다.

SSE를 선택한 이유는, 이전에 진행한 끄적끄적 프로젝트에서 알림 기능을 구현하며 SSE를 경험했기 때문입니다. 이번 프로젝트에서는 SSE를 더욱 깊이 이해하고, 다양한 상황에서의 적용 결과를 확인하고자 합니다.

구현 과정에서 발생하는 이슈와 해결 방법, 그리고 학습 내용을 문서화하여 기록으로 남길 예정입니다.

<br>

## 🛠 기술 스택

|      🖥️ Frontend      |        💯 Backend        |
| :-------------------: | :----------------------: |
|         React         |       Spring boot        |
|      Typescript       |           Java           |
| event-source-polyfill | Server-Sent Events (SSE) |
|           -           |          MySQL           |
|           -           |          Redis           |

<br>

## ⭐️ 목표

- 서버와 클라이언트 간 실시간 이벤트 처리 구조에 대해 심층적으로 학습하고, 이를 다양한 상황에서 분석 및 응용합니다.
- 다양한 환경에서 SSE를 활용한 알림 기능의 성능 및 동작 테스트를 진행합니다.

<br>

## 💡 고민할 점

- SSE는 서버 메모리에 데이터를 유지하는 특성이 있습니다. 만약 서버가 종료되었다가 다시 시작되면, 기존 연결 상태 및 데이터를 어떻게 복구하거나 처리할 수 있을까?
- 서버가 여러 대로 분산된 환경에서 SSE 연결을 어떻게 효율적으로 관리할까? 클라이언트 요청을 적절히 분배하고, 서버 간 상태 동기화를 유지하려면 어떤 전략이 필요할까?

<br>

---

## 🧑🏻‍💻 기록

1. [SSE 연결 프론트엔드, 백엔드 세팅](#1-sse-연결-프론트엔드-백엔드-세팅)
2. [SSE 연결 상태 캐시 도입](#2-sse-연결-상태-캐시-도입)
3. [메시지 브로커 Kafka 도입](#3-메시지-브로커-kafka-도입)

### [1. SSE 연결 프론트엔드, 백엔드 세팅]

[연결된 이슈](https://github.com/giwoong01/sse-study/issues/1)

SSE는 한 번 연결하면 계속 연결 상태를 유지합니다.
서버가 바뀌면서 메모리가 초기화되거나, 에러로 인해 연결이 끊기지 않는 이상 연결 상태는 유지됩니다.
그렇다면 이 연결 상태는 어떻게 관리할까요? 프론트는 연결 상태를 인지하기 어렵습니다. 서버에서 일방적으로 이벤트를 받을 뿐입니다.

제 생각에는 SSE 연결 상태는 서버에서 관리가 되어야 할 것 같습니다. 그 이유는 고민할 점에서 이야기한 부분과도 관련이 있습니다.
서버가 종료되었다가 다시 시작되면 메모리가 초기화되는데, 이는 레디스와 같은 캐시 스토리지로 연결 상태를 관리하면 좋을 것 같습니다.

프론트엔드에서 SSE 로직을 구현할 때 고민이 많았습니다. 커스텀 훅을 만들지, 함수형으로 만들지. 이 두가지 방법에는 각 장단점이 존재합니다.
커스텀 훅은 로직이 복잡하지만 재사용성과 가독성, 확장성면에서 좋습니다. 함수형은 로직이 간단하지만 확장이 어렵습니다.
그래서 저는 재사용성과 확장성을 고려해 커스텀훅을 선택했습니다.

커스텀 훅을 사용했을 때 특정 페이지에서는 커스텀 훅을 사용하고 싶지 않은 경우가 있어, 전체적으로 커스텀 훅을 사용하도록 강제하지 않아야합니다.

이 경우 3가지 방법이 존재합니다.

1. 조건적으로 커스텀 훅을 호출합니다.
2. 커스텀 훅으로 분리하되 각 페이지에서 필요할 때만 사용합니다.
3. SSE를 글로벌하게 처리해 필요한 컴포넌트만 구독합니다.

처음에는 2번과 같이 각 페이지에서 필요할 때만 커스텀 훅을 사용하려했습니다.
하지만 페이지를 이동할 때마다 해당 컴포넌트가 언마운트되므로 SSE 연결이 닫히고, 새 페이지에서 다시 SSE 연결이 이루어지는 구조가 됩니다.
즉, 매번 SSE 연결을 다시 생성했다가, 페이지 이동 시 언마운트되어 연결이 닫히는 일이 반복됩니다.

커스텀 훅을 사용한 방식은 페이지를 자주 이동하거나 SSE 연결 시간이 중요한 경우에는 비효율적일 수 있습니다.
특히 서버에서는 매번 새로운 SSE 클라이언트를 처리해야하므로 부하가 가증됩니다.

그렇기에 저는 Context API를 활용하려고 합니다. SSEProvider를 구현하여 특정 페이지를 제외하고 감싸줍니다.

```javascript
const App = () => {
  return (
    <BrowserRouter>
      <Routes>
        <Route path="/" element={<LoginPage />}></Route>
        <Route path="/register" element={<RegisterPage />}></Route>

        <Route
          path="/*"
          element={
            <SSEProvider>
              <Routes>
                <Route path="/home" element={<Home />} />
                <Route path="/dashboard" element={<Dashboard />} />
              </Routes>
            </SSEProvider>
          }
        ></Route>
      </Routes>
    </BrowserRouter>
  );
};
```

이로써 "/"를 제외한 모든 페이지에서 SSE를 관리하게됩니다.

```javascript
const { data } = useSSE();
```

그리고 특정 페이지에서 SSE를 구독하는 방법은 이와 같습니다.

Context API를 활용하여 SSE 관리를 함으로써 SSE연결은 전역적으로 유지가 되고,
필요한 컴포넌트에서만 SSE 구독이 가능합니다.

<img src="images/1.png" width="600" height="300"/>
<img src="images/2.png" width="320" height="300"/>

<br>

이와 같이 프론트엔드의 세팅을 어느정도 마무리하였습니다.
이미 백엔드에서 SSE 연동 로직을 구현을 해두었기때문에,
추가적으로 필요한 부분은 서버와 연동하면서 수정할 예정입니다.

### [2. SSE 연결 상태 캐시 도입]

[연결된 이슈](https://github.com/giwoong01/sse-study/issues/2)

SSE 연결을 어떻게 관리해야 할까요?

서버가 한 대뿐이라면 단순히 메모리에 저장해도 문제가 없겠지만,

- 서버가 여러 대인 경우 사용자가 A 서버에 SSE 연결된 상태여도 B 서버에서는 해당 연결 정보를 알 수 없음.
- 무중단 배포(블루-그린 배포) 등의 상황에서 서버가 재시작되면 기존의 SSE 연결 정보가 사라짐.

이 문제를 해결하기 위해 Redis를 캐시 저장소로 활용하여 SSE 연결 상태를 관리하려 합니다.

SSE 연결 상태 저장을 위해 Hash 구조를 선택합니다.

Hash 구조는 여러 필드를 하나의 키 아래에 저장할 수 있어, sse_connections라는 하나의 키 아래에 여러 emitterId와 serverId 쌍을 저장할 수 있습니다.
이와 같은 구조를 사용함으로써 관리가 용이하고, 특정 emitterId에 대한 serverId를 빠르게 조회할 수 있습니다.

Redis Hash를 사용하여 이와 같은 형태로 SSE 연결 정보를 저장합니다.

<img src="images/3.png" width="300" />

```
Key: sse_connections
Field (Key): emitterId  ("7")
Value: SERVER_ID        ("399a6ff8-e8ba-478f-8136-81a9699bf841")
```

이를 통해 특정 사용자의 SSE 연결이 어느 서버에 있는지 쉽게 조회할 수 있습니다.

하지만 연결 상태만 캐시 스토리지에 저장한다고 해서 멀티 서버에서의 알림 누락 문제는 해결되지 않습니다.
예를 들어, 사용자가 A 서버에 연결했지만 B 서버에서 알림을 보낸다면 A 서버는 이를 감지할 수 없습니다.

이를 해결하기 위해 메시지 브로커 Kafka를 도입하여 멀티 서버 환경에서도 문제 없이 알림이 가도록 해결해보도록 하겠습니다.

### [3. 메시지 브로커 Kafka 도입]

[연결된 이슈](https://github.com/giwoong01/sse-study/issues/3)

메시지 브로커인 Kafka를 도입하여 멀티 서버인 경우 테스트를 진행해보도록 하겠습니다.

Kafka는 분산 환경에서 안정적인 데이터 스트리밍을 지원하는 메시지 브로커입니다.
이를 활용하면 멀티 서버 환경에서도 효율적인 이벤트 전파와 메시지 처리가 가능합니다.

Kafka는 일반적으로 여러 서버에서 운영되지만, 이번 테스트에서는 단일 서버 환경에서 동작을 검증하겠습니다.

|               `동작과정`               |
| :------------------------------------: |
| <img src="images/4.png" width="600" /> |

1. Producer(A 서버)는 Redis에서 알림을 보낼 사용자의 serverId(B 서버)를 조회합니다.
2. Kafka Topic에 serverId를 Key로 설정하여 메시지를 전송합니다.
   - Topic 내부에서 serverId 별로 메시지를 분배합니다.
   - 같은 serverId를 가진 메시지는 동일한 서버에서 처리합니다.
3. Consumer(B 서버)는 B 서버로 전달된 메시지를 수신합니다.
   - 수신된 메시지는 SseEmitter를 통해 클라이언트에게 전달됩니다.

위 과정을 통해 Kafka를 활용한 메시지 전송이 어떻게 이루어지는지 확인할 수 있습니다.  
이제 배포된 환경에서 직접 테스트를 진행해 보겠습니다.

추가적으로 Kafka 설정 시, @KafkaListener와 설정 파일을 통해 Consumer Group(groupId)을 설정할 수 있습니다.
groupId는 Kafka Consumer Group을 식별하는 ID로, 메시지의 소비 방식에 영향을 줍니다.

다음 세 가지 상황을 배포된 환경에서 테스트할 예정입니다.

1. 같은 groupId를 사용할 경우 어떤 동작이 발생할까?
   - 여러 개의 서버가 같은 groupId를 가지면, 메시지를 서로 나눠서 처리하는지 확인해야 합니다.
   - 예를 들어, 두 개의 서버(A, B)가 같은 groupId를 가지고 있으면
     - 서버 A가 한 메시지를 처리하면, 서버 B는 해당 메시지를 받지 않는지 확인
     - 즉, 동일한 메시지를 중복 처리하는 경우가 없는지 테스트
2. 서버가 여러 개일 때 groupId의 역할은?
   - 여러 개의 서버가 같은 groupId를 가지면, Kafka가 메시지를 분산 처리하는지 확인해야 합니다.
   - 즉, 각 서버가 균등하게 메시지를 처리하는지 확인
   - (단, serverId가 Key로 설정되었기 때문에 같은 serverId를 가진 메시지는 특정 서버에서만 처리된다는 점을 고려)

이 2가지 상황과 Kafka 브로커가 잘 작동하는지 배포된 환경에서 멀티 서버를 두고 테스트 해보겠습니다.
